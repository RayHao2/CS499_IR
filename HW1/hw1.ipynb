{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HW1 Overview\n",
    "\n",
    "In this assignment, you will get familiar with basic document representation and analysis techniques. You will get the basic ideas of tokenization, stemming, normalization, constructing bag of words representation for text documents, and building an inverted index for efficient access.\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Set\n",
    "\n",
    "The instructor has prepared a small size collection of Yelp restaurant reviews (included in the provided sample Java project and separated into three folders) for this assignment. The review files are named and organized in the following manner:\n",
    "\n",
    "1. Each file contains all crawled review documents for a specific business on Yelp, and it is named by its unique ID on Yelp, e.g., *FAhx3UZtXvqNiEAd-GNruQ.json*;\n",
    "2. All the files are in json format. Each json file contains a json array of reviews ('Reviews') and a json object about the information of the restaurant ('RestaurantInfo').              \n",
    "\n",
    "\t2.1 The json object for **user review** is defined as follows:           \n",
    "\t\t{          \n",
    "\t\t\t'Author':'author name (string)',\n",
    "\t\t\t'ReviewID':'unique review id (string)',  \n",
    "\t\t\t'Overall':'numerical rating of the review (float)',\n",
    "\t\t\t'Content':'review text content (string)',   \n",
    "\t\t\t'Date':'date when the review was published',   \n",
    "\t\t\t'Author_Location':'author's registered location'  \n",
    "\t\t} \n",
    "    \n",
    "\t2.2 The json object for **restaurant info** is defined as follows:                                \n",
    "\t\t{                \n",
    "\t\t\t'RestaurantID':'unique business id in Yelp (string)',    \n",
    "\t\t\t'Name':'name of the business (string)',      \n",
    "\t\t\t'Price':'Yelp defined price range (string)',    \n",
    "\t\t\t'RestaurantURL':'actual URL to the business page on Yelp (string)',   \n",
    "\t\t\t'Longitude':'longitude of the business (double)',              \n",
    "\t\t\t'Latitude':'latitude of the business (double)',              \n",
    "\t\t\t'Address':'address of the business (string)',       \n",
    "\t\t\t'ImgURL':'URL to the business's image on Yelp (string)'     \n",
    "\t\t} \n",
    "\n",
    "In the following discussion, we will refer to each individual user review as a **document**. \n",
    "Note that some collected json files might not strictly follow the above json object definitions, e.g., some fields are missing or empty. As a result, properly handling the exceptions in json parsing is necessary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "from nltk.tokenize import word_tokenize\n",
    "import nltk\n",
    "from string import punctuation\n",
    "from nltk.stem import PorterStemmer\n",
    "import re\n",
    "import sys\n",
    "import matplotlib.pyplot as plt\n",
    "from elasticsearch import Elasticsearch\n",
    "import time\n",
    "def is_integer(s):\n",
    "    try:\n",
    "        int(s)\n",
    "        return True\n",
    "    except ValueError:\n",
    "        return False\n",
    "\n",
    "\n",
    "def is_float(s):\n",
    "    try:\n",
    "        float(s)\n",
    "        return True\n",
    "    except ValueError:\n",
    "        return False\n",
    "\n",
    "\n",
    "def is_fraction(s):\n",
    "    fraction_pattern = re.compile(r'^\\d+/\\d+$')\n",
    "    return bool(fraction_pattern.match(s))\n",
    "\n",
    "\n",
    "def remove_non_alphanumeric(input_string):\n",
    "    # Use regular expression to keep only English letters and numbers\n",
    "    result = re.sub(r'[^a-zA-Z0-9]', '', input_string)\n",
    "    return result\n",
    "\n",
    "def is_stop_word(token):\n",
    "    stop_words = set([\n",
    "        'a', 'an', 'and', 'are', 'as', 'at', 'be', 'but', 'by', 'for', 'if', 'in',\n",
    "        'into', 'is', 'it', 'no', 'not', 'of', 'on', 'or', 'such', 'that', 'the',\n",
    "        'their', 'then', 'there', 'these', 'they', 'this', 'to', 'was', 'will', 'with'\n",
    "    ])\n",
    "\n",
    "    # Convert the word to lowercase for case-insensitive comparison\n",
    "    lowercase_word = token.lower()\n",
    "\n",
    "    return lowercase_word in stop_words\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Basic Text Processing\n",
    "\n",
    "Apply NLTK library (http://www.nltk.org/install.html). Recall the following steps to pre-process the document: tokenization, normalization, stemming, stopwords removal.\n",
    "\n",
    "- Tokenization: tokenize the review content of each document into tokens.\n",
    "- Normalization: normalize the tokens from step 1, by removing individual punctuation marks (here is a list of [English punctuation marks](http://en.wikipedia.org/wiki/Punctuation_of_English)), converting tokens into lower cases, and recognizing digit numbers, e.g., integers and floats, and map them to a special symbol \"NUM\". \n",
    "- Stemming: stem the tokens back to their root form. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Understand Zipf's Law (50pts)\n",
    "\n",
    "First, let's validate Zipf's law with the provided Yelp review data sets. This can be achieved by the following steps:\n",
    "\n",
    "1. Process the text document according to the discussed steps above.\n",
    "2. For each token, go over all the review documents containing it, and accumulate its frequency in those documents, i.e., total term frequency (TTF).\n",
    "3. Order the tokens by their TTF in descending order.\n",
    "4. Create a dot plot by treating each word's rank as x-axis and its TTF as y-axis. Please use log-log scale for this plot.\n",
    "\n",
    "*Hint: basically, you can maintain a look-up table in memory while you are scanning through the documents, so that you only need to go through the corpus once to get the counts for all the tokens. In the provided implementation, this look-up table is maintained in the \"Corpus\" class, named \"m_dictionary\".*\n",
    "\n",
    "From the resulting plot, can we find a strong linear relationship between the x and y axes in the log-log scale? If so, what is the slope of this linear relationship? To answer these questions, you can dump the data into Excel and use the plot function there, or use some scientific computing packages in python for curve fitting for this purpose.         \n",
    "\n",
    "Then change the counting statistics in the previous experiment from *total term frequency (TTF)* to *document frequency (DF)* (i.e., how many documents contain this specific term), and perform the experiment again. According to new curve and corresponding slope and intercept of the linear interpretation, can you conclude which counting statistics, i.e., *TTF* v.s., *DF*, fits Zipf's law better on this data set? Can you give any explanation about why it fits the law better?\n",
    "\n",
    "**What to submit**: \n",
    "\n",
    "1. Paste your implementation of text normalization module. (15pts)\n",
    "2. Two curves in log-log space generated above, with the corresponding slopes and intercepts of the linear interpretation results. (20pts)\n",
    "3. Your answers and thoughts to the above questions. (15pts)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Building An Inverted Index (50pts)\n",
    "\n",
    "An inverted index accelerates text corpus access by building a term to document mapping. In this task, we will compare efficiency speed-up provided by such a specialized data structure. We will apply Elasticsearch(Python) - https://pypi.python.org/pypi/elasticsearch to build the index and query it. Check the [docs](https://elasticsearch-py.readthedocs.io/en/v8.12.0/) and these [examples](https://www.elastic.co/guide/en/elasticsearch/client/python-api/master/examples.html#ex-index). To query a document, we can first use ES to query with the API, such as \n",
    "> \"match\": {\"\\<filed\\>\":\"query keywords\"}. \n",
    "\n",
    "This should be re with its default BM25 retrieval model. Then implement Bag-of-Word retrieval model without using ES query.\n",
    "\n",
    "Use these two retrieval methods, i.e., inverted index and brute-force search over your BoW document representation, to count the total number of documents that match with the 10 queries listed below accordingly,\n",
    "\n",
    "\tgeneral chicken\n",
    "\tfried chicken\n",
    "\tBBQ sandwiches\n",
    "\tmashed potatoes\n",
    "\tGrilled Shrimp Salad\n",
    "\tlamb Shank\n",
    "\tPepperoni pizza\n",
    "\tbrussel sprout salad\n",
    "\tFRIENDLY STAFF\n",
    "\tGrilled Cheese\n",
    "\n",
    "Please record the total running time of each of retrieval methods, and the number of returned documents accordingly.\n",
    "\n",
    "**What to submit**: \n",
    "\n",
    "1. Paste your implementation of Bag-of-Word document representation (e.g., how to construct it from raw document content, and how to search for a particular term in it). (15pts)\n",
    "2. Two curves in log-log space generated by using Elasticsearch inverted index for collecting TTF and DF statistics, with the corresponding running time statistics in comparison with your implementation in problem one. (15pts)\n",
    "3. Running time and total number of returned documents by two retrieval models. If you found these two methods gave you different number of matched documents, do you have any explanation about it? (20pts)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extra Credits (10pts)\n",
    "\n",
    "You are encouraged to further investigate Zipf's law. For example (but not limited to), subsample the reviews with different sizes and see if an increased number of documents better help us verify Zipf's law by comparing the log-log curves.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Submission\n",
    "\n",
    "This assignment has in total 100 points. The deadline is Feb 6 23:59 PDT. You should submit your report in **PDF** using the homework latex template, and submit your code (notebook). "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
