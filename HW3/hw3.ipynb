{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HW3 Overview\n",
    "\n",
    "\n",
    "In this assignment, we will study representations by vector space model, language model and neural networks, including TF-IDF and word embedding by word2vec.\n",
    "\n",
    "We will reuse the same Yelp dataset and refer to each individual user review as a **document**. You should reuse your JSON parser in this assignment.\n",
    "\n",
    "The same pre-processing steps you have developed in HW1 will be used in this assignment, i.e., tokenization, stemming, normalization and stopword removal. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word Embedding (50 points)\n",
    "\n",
    "Use [genism](https://radimrehurek.com/gensim/models/word2vec.html) library, download a pre-trained model or train a word2vec model (10 epochs) using the review data. Then use the model to get the vector representations of the document and query.\n",
    "\n",
    "Hint: You can average the embedding of the terms to get the vector representation \n",
    "for a document.\n",
    "\n",
    "Use the following 10 queries (same as the ones in HW1) and retrieve the top 3 documents for each query based on cosine similarity:\n",
    "\n",
    "\tgeneral chicken\n",
    "\tfried chicken\n",
    "\tBBQ sandwiches\n",
    "\tmashed potatoes\n",
    "\tGrilled Shrimp Salad\n",
    "\tlamb Shank\n",
    "\tPepperoni pizza\n",
    "\tbrussel sprout salad\n",
    "\tFRIENDLY STAFF\n",
    "\tGrilled Cheese\n",
    "\n",
    "Note that the training does not require GPU -- moderate CPU is good enough.  If your computation power is limited, i.e., limited memory or cpu, you can choose to download a pre-trained model or train the model on partial data, e.g., 50% or fewer review data. Please document the corresponding training detail in your report.\n",
    "\n",
    "**What to submit**:\n",
    "\n",
    "1. Paste your implementation of document representation and cosine similarity calculation. Report the training time of the word embedding model.\n",
    "2. For the top 3 documents of each query, print the document and its cosine similarity score.\n",
    "3. For the first three queries, analyze the relation between relevance and cosine similarity score: is a high score document more relevant to the query?\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TF-IDF (50 points)\n",
    "Use the [gensim TF-IDF](https://radimrehurek.com/gensim/models/tfidfmodel.html) to estimate a TF-IDF model. Represent the document and query by TF-IDF vector and repeat the tasks in Word Embedding section.\n",
    "\n",
    "**What to submit**:\n",
    "1. Paste your implementation of document representation. Report the training time of the model.\n",
    "2. For the top 3 documents of each query, print the document and its cosine similarity score.\n",
    "3. For the first three queries, analyze the relation between relevance and cosine similarity score: is a high score document more relevant to the query?\n",
    "4. Compare TF-IDF with averaged word2vec, which model gives a better document representation and query representation? Why?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import TfidfModel\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extra Credits (5pts)\n",
    "\n",
    "You are encouraged to further investigate other document representation methods, e.g., [doc2vec](https://radimrehurek.com/gensim/auto_examples/tutorials/run_doc2vec_lee.htm). Is it harder to train? Does it perform better than the two models we investigated?\n",
    "\n",
    "# Submission\n",
    "\n",
    "This assignment has in total 100 points. The deadline is June 9th 23:59 PDT. You should submit your report in **PDF** using the homework latex template and submit your code (notebook). "
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
